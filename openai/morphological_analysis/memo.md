# 学習メモ

[ChatGPT の仕組み](https://zenn.dev/umi_mori/books/chatbot-chatgpt/viewer/chatgpt_mechanism)

- 形態素解析（Morphological Analysis）:自然言語で書かれた文を言語上の最小単位である形態素に分割し、それぞれの品詞や変化などを割り出すこと。IT の分野ではコンピュータによる自然言語処理の一つとして、かな漢字変換や全文検索、機械翻訳などで用いられる。

  - 例えば、「東京で週末を過ごす」という文章があった時、以下のように分割することができ る。
  - 東京：固有名詞・地名
  - で：格助詞
  - 週末：普通名詞
  - を：格助詞
  - 過ごす：動詞・サ行五段・終止形

- MeCab: 京都大学情報学研究科と日本電信電話株式会社コミュニケーション科学基礎研 究所の共同研究ユニットプロジェクトによって開発された、オープンソースの形態素解析エンジンである。

- AI に関する深い知識:

  - 生成系 AI の根幹にある「敵対的生成ネットワーク（GAN）」や時系列的特徴を抽出することができる「Transformer」と呼ばれる深層学習モデルへの理解が重要です。
  - さらに「強化学習」に関する知識も身につけると、ChatGPT のような RLHF のアルゴリズムベースのモデルも性能改善しやすくなるでしょう。

- RLHF(Reinforcement Learning From Human Feedback):言語モデルに対して、人間のつけたラベル（フィードバック）からの強化学習でファインチューニングするアルゴリズム
- SFT（Supervised Fine-Tuning）: 教師データを用いて、ファインチューニングするアルゴリズム
- PPO（Proximal Policy Optimization）: 方策の最適化をする強化学習アルゴリズム（OpenAI が開発）
- ファインチューニング: モデルのパラメーターを微調整して、性能を向上させる手法
- 方策（ポリシー）: 強化学習モデルの中のエージェントが取る行動のルール（特定の状況においてエージェントが取る行動の確率分布）

- ChatGPT の学習の中で登場する主なデータセットは次の 3 つです。

  - 大規模なコーパス: 自然言語の文章を集約したデータベース
  - 会話データセット: 入力となる自然言語（プロンプト）と、それに対して人が回答した自然言語（レスポンス）のデータ
  - 評価データセット: 入力となる自然言語に対する回答が妥当な文章かを人が評価したデータ

- GPT（Generative Pre-trained Transformer）: OpenAI が開発した「文章生成モデル」です。
- RM（Reward Model、報酬モデル）: 「文章の良さを評価するモデル」です（ChatGPT における文脈）。

### ChatGPT の学習プロセス

- GPT-3.5 の事前学習
  - まず GPT-3.5 の事前学習をします。
  - GPT-3.5 に対して、Web から取得した大規模なコーパスを用いて、学習させます。
- GPT-3.5 のファインチューニング（SFT）
  - 次に、人の会話データセットを用いて、事前学習した GPT-3.5 のファインチューニングをします。
  - このファインチューニングされたモデルは、SFT モデルと呼ばれます。
- RM の学習
  - 3 つ目に、RM の学習をさせます。
  - まず評価データセットを作るために、SFT モデルを用いて、1 つの入力データから「複数の出力データ」を生成します。
  - そして、出力結果に対して、「良い文章かどうか」に基づいて、人がランキングをつけて評価します。
  - この評価データセットを用いて、RM を学習させます。
- RM による SFT モデルの最適化（PPO）
  - 最後に、RM による SFT モデルの最適化をします。
  - まず作成した SFT モデルを用いて、入力データから出力データを生成します。
  - この出力データに対して、RM が評価を行い、評価が高くなるように、PPO を用いて方策を最適化します。
